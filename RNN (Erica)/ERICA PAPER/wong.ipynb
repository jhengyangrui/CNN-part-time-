{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 0  loss: 0.291577\n",
      "Number of iterations: 25  loss: 0.153137\n",
      "Number of iterations: 50  loss: 0.152862\n",
      "Number of iterations: 75  loss: 0.151834\n",
      "Number of iterations: 100  loss: 0.151459\n",
      "Number of iterations: 125  loss: 0.151625\n",
      "Number of iterations: 150  loss: 0.15114\n",
      "Number of iterations: 175  loss: 0.144482\n",
      "model_save:  model_save2\\modle.ckpt\n",
      "The train has finished\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "rnn_unit=10       #隱藏數量(自訂)\n",
    "input_size=7 #BICPRFM\n",
    "output_size=1 #YN\n",
    "lr=0.0001         #學習率\n",
    "#——————————————————import data ——————————————————————\n",
    "f=open('wong_bicprfm.csv')\n",
    "df=pd.read_csv(f)     #讀入資料\n",
    "data=df.iloc[:,3:11].values  #取第3-10列(取頭不取尾)\n",
    "\n",
    "\n",
    "#獲取訓練集 (做標準化)\n",
    "def get_train_data(batch_size=10,time_step=5,train_begin=0,train_end=3100):\n",
    "    batch_index=[]\n",
    "    data_train=data[train_begin:train_end]\n",
    "    normalized_train_data=(data_train)/np.mean(data_train,axis=0)  #標準化\n",
    "    train_x,train_y=[],[]   #訓練集\n",
    "    for i in range(len(normalized_train_data)-time_step):\n",
    "       if i % batch_size==0:\n",
    "           batch_index.append(i)\n",
    "       x=normalized_train_data[i:i+time_step,:7]\n",
    "       y=data_train[i:i+time_step,7,np.newaxis]\n",
    "       train_x.append(x.tolist())\n",
    "       train_y.append(y.tolist())\n",
    "    batch_index.append((len(normalized_train_data)-time_step))\n",
    "    return batch_index,train_x,train_y\n",
    "\n",
    "\n",
    "#測試集\n",
    "def get_test_data(time_step=5,test_begin=3100):\n",
    "    data_test=data[test_begin:]\n",
    "    mean=np.mean(data_test,axis=0)\n",
    "    std=np.std(data_test,axis=0)\n",
    "    normalized_test_data=data_test/mean  #標準化\n",
    "    size=(len(normalized_test_data)+time_step-1)//time_step  #有size個sample\n",
    "    test_x,test_y=[],[]\n",
    "    for i in range(size-1):\n",
    "       x=normalized_test_data[i*time_step:(i+1)*time_step,:7]\n",
    "       y=data_test[i*time_step:(i+1)*time_step,7]\n",
    "       test_x.append(x.tolist())\n",
    "       test_y.extend(y)\n",
    "    test_x.append((normalized_test_data[(i+1)*time_step:,:7]).tolist())\n",
    "    test_y.extend((data_test[(i+1)*time_step:,7]).tolist())\n",
    "    return mean,std,test_x,test_y\n",
    "\n",
    "\n",
    "#——————————————————定義神經網絡變量——————————————————\n",
    "#輸入層  輸出層權重  bias\n",
    "\n",
    "weights={\n",
    "         'in':tf.Variable(tf.random_normal([input_size,rnn_unit])),\n",
    "         'out':tf.Variable(tf.random_normal([rnn_unit,1]))\n",
    "        }\n",
    "biases={\n",
    "        'in':tf.Variable(tf.constant(0.1,shape=[rnn_unit,])),\n",
    "        'out':tf.Variable(tf.constant(0.1,shape=[1,]))\n",
    "       }\n",
    "\n",
    "#——————————————————定義lstm——————————————————\n",
    "def lstm(X):\n",
    "    \n",
    "    batch_size=tf.shape(X)[0]\n",
    "    time_step=tf.shape(X)[1]\n",
    "    \n",
    "    w_in=weights['in']\n",
    "    b_in=biases['in']\n",
    "    input=tf.reshape(X,[-1,input_size])  #需要将tensor转成2维进行计算，计算后的结果作为隐藏层的输入\n",
    "    input_rnn=tf.matmul(input,w_in)+b_in\n",
    "    input_rnn=tf.reshape(input_rnn,[-1,time_step,rnn_unit])  #将tensor转成3维，作为lstm cell的输入\n",
    "    \n",
    "    cell=tf.nn.rnn_cell.BasicLSTMCell(rnn_unit)\n",
    "    init_state=cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    \n",
    "    output_rnn,final_states=tf.nn.dynamic_rnn(cell, input_rnn,initial_state=init_state, dtype=tf.float32)\n",
    "    output=tf.reshape(output_rnn,[-1,rnn_unit]) \n",
    "    w_out=weights['out']\n",
    "    b_out=biases['out']\n",
    "    \n",
    "    pred=tf.matmul(output,w_out)+b_out\n",
    "    return pred,final_states\n",
    "\n",
    "#————————————————訓練模型————————————————————\n",
    "\n",
    "def train_lstm(batch_size=10,time_step=5,train_begin=0,train_end=3100):\n",
    "    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])\n",
    "    Y=tf.placeholder(tf.float32, shape=[None,time_step,output_size])\n",
    "    batch_index,train_x,train_y=get_train_data(batch_size,time_step,train_begin,train_end)\n",
    "    with tf.variable_scope(\"sec_lstm\"):\n",
    "        pred,final_states=lstm(X)\n",
    "    loss=tf.reduce_mean(tf.square(tf.reshape(pred,[-1])-tf.reshape(Y, [-1])))\n",
    "    train_op=tf.train.RMSPropOptimizer(lr).minimize(loss)\n",
    "    saver=tf.train.Saver(tf.global_variables(),max_to_keep=5) #保存模型\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(200):     #迭代次數\n",
    "            for step in range(len(batch_index)-1):\n",
    "                _,loss_=sess.run([train_op,loss],feed_dict={X:train_x[batch_index[step]:batch_index[step+1]],Y:train_y[batch_index[step]:batch_index[step+1]]})\n",
    "            if i % 25 == 0:\n",
    "                print(\"Number of iterations:\",i,\" loss:\",loss_)\n",
    "            \n",
    "        print(\"model_save: \",saver.save(sess,'model_save2\\\\modle.ckpt'))\n",
    "        print(\"The train has finished\")\n",
    "train_lstm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_save2\\modle.ckpt\n",
      "The accuracy of this predict: 0.703773584906\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#————————————————預測模型————————————————————\n",
    "def prediction(time_step=5):\n",
    "    X=tf.placeholder(tf.float32, shape=[None,time_step,input_size])\n",
    "    mean,std,test_x,test_y=get_test_data(time_step)\n",
    "    with tf.variable_scope(\"sec_lstm\",reuse=True):\n",
    "        pred,_=lstm(X)\n",
    "    saver=tf.train.Saver(tf.global_variables())\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        module_file = tf.train.latest_checkpoint('model_save2')\n",
    "        saver.restore(sess, module_file)\n",
    "        test_predict=[]\n",
    "        for step in range(len(test_x)-1):\n",
    "          prob=sess.run(pred,feed_dict={X:[test_x[step]]})\n",
    "          predict=prob.reshape((-1))\n",
    "          test_predict.extend(predict)\n",
    "        test_y=np.array(test_y)\n",
    "        test_predict=np.array(test_predict)     \n",
    "        test_predict_2 = []\n",
    "        for ans in test_predict:\n",
    "            if ans >= 0.5:\n",
    "                test_predict_2.append(1)\n",
    "            else:\n",
    "                test_predict_2.append(0)\n",
    "        test_predict = test_predict_2\n",
    "        acc=1-sum(np.abs(test_predict-test_y[:len(test_predict)]))/len(test_predict)  #偏差程度\n",
    "        print(\"The accuracy of this predict:\",acc)\n",
    "\n",
    "        numpy.savetxt('predit_wong.csv',test_predict , delimiter = ',')\n",
    "\n",
    "prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
